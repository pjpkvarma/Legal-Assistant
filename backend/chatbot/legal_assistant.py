from fastapi import APIRouter, Form
import requests

# Initialize the router for chatbot endpoints
router = APIRouter()

def call_llm(prompt: str) -> str:
    """
    Sends a prompt to the locally running LLaMA2 model via Ollama
    and returns the generated response.

    Args:
        prompt (str): The user query or instruction to be sent to the model.

    Returns:
        str: The LLM-generated response.
    """
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": "llama2", "prompt": prompt, "stream": False},
            timeout=30
        )
        response.raise_for_status()
        return response.json()["response"].strip()
    except requests.exceptions.RequestException as e:
        return f"Error contacting LLM backend: {e}"

@router.post("/chat/")
def legal_assistant_chat(message: str = Form(...)):
    """
    Chat endpoint for the Legal Assistant chatbot.

    Accepts a user's legal question and returns a response generated by LLaMA2.

    Args:
        message (str): The user's question or prompt.

    Returns:
        dict: JSON containing the chatbot's response.
    """
    system_prompt = (
        "You are Legal Assistant, an AI trained to answer legal questions in simple, "
        "non-biased language. Respond clearly and concisely.\n\n"
        f"User's question:\n{message}"
    )
    
    reply = call_llm(system_prompt)
    return {"response": reply}
